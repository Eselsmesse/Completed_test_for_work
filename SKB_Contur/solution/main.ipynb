{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание проекта\n",
    "\n",
    "* Бизнес задача\n",
    "\n",
    "Разработать модель, которая будет способна по паре `текст документа` и `пункт анкеты` извлекать из текста документа нужный фрагмент текста.  \n",
    "\n",
    "* Этапы исполнения\n",
    "\n",
    "* * Предподготовка данных\n",
    "* * Исследовательская деятельность\n",
    "* * Обучение моделей\n",
    "* * Тестирование\n",
    "* * Ход исследования\n",
    "* * Формирование ответов тестового файла "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nerlib\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "\n",
    "Предварительно разбиты на train и valid выборки в соотношении 1 к 5 (def create_train_valid) и переведены в формат .spacy для дальнейшего применение при обучении модели (def prepare_training), сохранены в папку data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировачной выборки: 1467\n",
      "Размер валидационной выборки: 359\n"
     ]
    }
   ],
   "source": [
    "# формируем train/valid выборки\n",
    "train_data, valid_data = nerlib.create_train_valid('data/main_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 471/1467 [00:02<00:05, 193.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 551/1467 [00:03<00:05, 179.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 593/1467 [00:03<00:05, 164.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 630/1467 [00:03<00:04, 168.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 702/1467 [00:04<00:03, 208.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 751/1467 [00:04<00:03, 194.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 873/1467 [00:04<00:02, 236.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 932/1467 [00:05<00:02, 257.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 990/1467 [00:05<00:02, 233.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1064/1467 [00:05<00:01, 227.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1117/1467 [00:05<00:01, 229.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1188/1467 [00:06<00:01, 222.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1238/1467 [00:06<00:01, 219.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1285/1467 [00:06<00:00, 192.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1336/1467 [00:06<00:00, 217.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1406/1467 [00:07<00:00, 275.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1467/1467 [00:07<00:00, 200.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 25/359 [00:00<00:02, 120.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 77/359 [00:00<00:01, 163.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 112/359 [00:00<00:01, 166.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 181/359 [00:01<00:00, 209.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 231/359 [00:01<00:00, 227.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 313/359 [00:01<00:00, 254.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:01<00:00, 210.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# конвертируем данные в Doc формат\n",
    "\n",
    "train_model = nerlib.prepare_training(train_data)\n",
    "#train_model.to_disk('./data/train_data.spacy')\n",
    "valid_model = nerlib.prepare_training(valid_data)\n",
    "#valid_model.to_disk('./data/valid_data.spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание Word2Vec модели"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведенные эксперименты показали что добавление кастомных векторов увеличивает accuracy в среднем на 0.13 поэтому создадим word2vec модель с помощью библиотеки gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тренируем word_2_vec модель и сохраняем в папку models с соответствующим названием\n",
    "nerlib.training('contur_train_count_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('заключаемого', 0.972634494304657), ('отдельно', 0.9699167013168335), ('прилагается', 0.9640468955039978), ('смп', 0.9606159329414368), ('заявке', 0.9573271870613098), ('значения', 0.9514512419700623), ('файлом', 0.9512737393379211), ('информационная', 0.9503987431526184), ('которым', 0.9493719339370728), ('отдельным', 0.9490988254547119)]\n"
     ]
    }
   ],
   "source": [
    "# отображения ближайших векторов к слову \"контракт\"\n",
    "nerlib.gensim_similary('контракт', 'word_vectors/word2vec_contur_train_count_5.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Инициализация векторов\n",
    "\n",
    "Нам необходимо создать \"пустую\" модель с нашими кастомными векторами (файл .txt) из папка word_vectors\n",
    "\n",
    "Инициализация проводится консольной командой:\n",
    "\n",
    "!python -m spacy init vectors ru word_vectors/word2vec_contur_train_count_5.txt --name word_vectors_model models/word_vectors_model \n",
    "\n",
    "2. Добавляем 'ner' в пайплайн обучения нашей \"пустой модели\"\n",
    "\n",
    "3. Обучение модели\n",
    "\n",
    "Указываем расположение подготовленных ранее данных для обучения и валидации а также txt файл с нашими векторами и запускаем команду:\n",
    "\n",
    "!python -m spacy train models/word_vectors_model/config.cfg --output models/05 --paths.train data/train_data.spacy --paths.dev data/valid_data.spacy --paths.vectors models/word_vectors_model\n",
    "\n",
    "4. Результаты обучения\n",
    "\n",
    "<image src=\".\\images\\Процесс обучения.png\" alt=\"Процесс обучения\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем вектора\n",
    "!python -m spacy init vectors ru word_vectors/word2vec_contur_train_count_5.txt --name word_vectors_model models/word_vectors_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем только что обученную модель\n",
    "nlp = spacy.load('models/word_vectors_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем извлечение именованных сущностей в пайплайн модели\n",
    "nlp.add_pipe('ner', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем модель с измененным пайплайном\n",
    "spacy.to_disk('models/word_vectors_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем NER модель\n",
    "!python -m spacy train models/word_vectors_model/config.cfg --output models/05 --paths.train data/train_data.spacy --paths.dev data/valid_data.spacy --paths.vectors models/word_vectors_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование\n",
    "\n",
    "Для воспроизведения работы модели необходимо импортировать библиотеку spacy для работы модели и запустить приведенный ниже код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем обученную модель\n",
    "import spacy\n",
    "nlp = spacy.load('models/05/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем датасет для оценки точности\n",
    "valid = pd.DataFrame(valid_data, columns=['text', 'extracted_part'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем колонку с текстом\n",
    "valid['extracted_text'] = valid.apply(lambda row: nerlib.extract_text(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем колонку ответами\n",
    "valid['predict'] = valid['text'].apply(lambda row: str(nerlib.predict_text(row, 'models/05/model-best')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7325905292479109"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# точность предсказанного текста на valid выборке\n",
    "accuracy_score(valid['predict'], valid['extracted_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ход исследования\n",
    "\n",
    "Базовая модель давала точность в 0.65, попытки улучшить качество с помощью техники upsample и соответственно перевод в нижний регистр не дали прироста, а только ухудшили метрику, в свою очередь добавление кастомных векторов увеличило метрику в среднем на 0.12 пунктов, далее я тестировал оубчение модели с различными гиперпараметрами, которые можно посмотреть в файле `.\\models\\05\\model-best\\config.cfg`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формирование ответов для тестового файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем тестовые данные\n",
    "test = pd.read_json('data/main_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# извлекаем часть текста\n",
    "test['extracted_part'] = test['text'].apply(lambda row: nerlib.predict_test_text(row, 'models/05/model-best'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем файл с ответами\n",
    "test.to_json('data/predictions.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Возможные улушения\n",
    "\n",
    "* Вероятно существуют и другие подходы к решению задачи NER, но иструмент spacy мне показался самым удобным, простым в использвании и в воспроизводимости.\n",
    "\n",
    "* Использование архитектур нейронных сетей для ансамблирование ответов\n",
    "\n",
    "* Оптимизировать код и файловый менеджмент.\n",
    "\n",
    "* Обернуть модель в интерактивную среду.\n",
    "\n",
    "P.S. При визуальной оценке на валидационных данных было замечено что некоторые части тренировочного датасета размечены не полностью, а именно в некоторых примерах после суммы упущено сокращение \"руб\", а модель в свою очередь захватывает это сокращение, что является более полной формой извлекаемого фрагмента, но при оценке через accuracu_score все же даёт нам неправлиьный ответ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Спасибо за уделенное время, прошу предоставить фидбек по данной работе**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6d6c976c786deb466a5ae54b150c4850c7d20dc4901b119448dee48fdfed792"
  },
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
