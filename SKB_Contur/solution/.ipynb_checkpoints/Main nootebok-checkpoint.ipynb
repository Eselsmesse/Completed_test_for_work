{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827bca48",
   "metadata": {},
   "source": [
    "# Check list\n",
    "\n",
    "1. Create train (text, enteties) / valid set (text, enteties, extracted_text) **done**\n",
    "\n",
    "2. Create Score functional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0ae7b",
   "metadata": {},
   "source": [
    "# Введение\n",
    "В Контуре мы много работаем с документами: арбитражные иски, госзакупки, исполнительные производства. В данном задании мы предлагаем вам сделать модель, которая поможет отделу госзакупок извлекать \n",
    "нужный кусок текста из документа для того, чтобы сформировать анкету заявки. То, какой именно фрагмент текста нужно извлечь, зависит от пункта анкеты, соответствующего документу.\n",
    "Всего в каждом документе, с которыми вы будет работать, есть 1 из 2-х пунктов анкеты, по которым необходимо извлекать кусочки из текста:\n",
    "- обеспечение исполнения контракта\n",
    "- обеспечение гарантийных обязательств\n",
    "\n",
    "Соответственно, ваша модель, принимая на вход `текст документа` и `наименование одного из двух пунктов`, должна возвращать `соответствующий кусочек текста из текста документа`.\n",
    "\n",
    "# Данные\n",
    "\n",
    "### train.json \n",
    "Данные для обучения в формате json имеют следующие поля:\n",
    "- `id`: int - id документа\n",
    "-  `text`: str - текст документа, в котором может содержаться фрагмент текста, соответствующий пункту анкеты из поля `label`\n",
    "- `label`: str - название пункта анкеты. Может принимать одно из двух значений: `обеспечение исполнения контракта` или `обеспечение гарантийных обязательств`\n",
    "- `extracted_part`: dict следующего формата:\n",
    "    ```\n",
    "    {\n",
    "        'text': [фрагмент текста из поля `text`, соответствующий пункту анкеты], \n",
    "        'answer_start': [индекс символа начала фрагмента текста в тексте документа],\n",
    "        'answer_end': [индекс символа конца фрагмента текста в тексте документа]\n",
    "    }\n",
    "   ```\n",
    "  \n",
    "### test.json\n",
    "\n",
    "Для демонстрации работы модели используйте данные из файла `test.json`. В нем есть все те же поля, что и в файле `train.json`, кроме поля `extracted_part` - именно его вам и нужно будет добавить,\n",
    "для того, чтобы мы смогли оценить качество вашей модели.\n",
    "\n",
    "# Тестовое задание\n",
    "\n",
    "Для выполнения тестового задания требуется разработать модель, которая будет способна по паре `текст документа` и `пункт анкеты` извлекать из текста документа нужный фрагмент текста. \n",
    "Обучив модель, добавьте в файл `test.json` поле `extracted_part` в том же формате, что и в файле `train.json`. Новый файл назовите `predictions.json`\n",
    "\n",
    "**Подсказка**: изучив данные, вы можете заметить, что у части наблюдений отсутствует фрагмент текста к извлечению (пустая строка внутри поля `extracted_part` с `answer_start` и\n",
    "`answer_end` равными нулю). Это означает, что в тексте документа нет нужного фрагмента текста, соответствующего пункту анкеты. Учтите это в обучении вашей модели и при формировании\n",
    "файла с ответами.\n",
    "\n",
    "# Критерии оценки\n",
    "1. Для оценки финального решения будет использоваться метрика `Accuracy`: доля наблюдений, в которых извлеченный моделью фрагмент текста полностью соответствует фактически\n",
    "   требуемому фрагменту.\n",
    "2. Чистота кода, оформление и понятность исследования.\n",
    "\n",
    "# Требования к решению\n",
    "В качестве решения мы ожидаем zip-архив со всеми *.py и *.ipynb файлами в папке solution и файлом `predictions.json` в корне. Формат имени zip-архива: LastName_FirstName.zip (пример Ivanov_Ivan.zip).\n",
    "Файл `predictions.json` должен включать в себя колонки `id`, `text`, `label`, содержащие те же данные, что и исходный файл `test.json`, а также колонку `extracted_part` в том же\n",
    "формате, что и в файле `train.json`\n",
    "Разметка тестового набора данных и включение его в обучение/валидацию запрещены.\n",
    "\n",
    "В папке solution должно быть отражено исследование и весь код, необходимый для воспроизведения исследования.\n",
    "\n",
    "Успехов!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c083ff96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# based\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.tokens import Doc, DocBin\n",
    "from collections import Counter\n",
    "from spacy.training.example import Example\n",
    "\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "#sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd03167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    \n",
    "    '''\n",
    "    Description:\n",
    "    Функция чтения данных\n",
    "    \n",
    "    input:\n",
    "    file -> path to json file\n",
    "    \n",
    "    output:\n",
    "    data -> json file\n",
    "    '''\n",
    "    \n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def save_data(file, data):\n",
    "    '''\n",
    "    Description:\n",
    "    Функция сохранение данных\n",
    "    \n",
    "    input:\n",
    "    file -> directory for save json file\n",
    "    \n",
    "    output:\n",
    "    data -> json file\n",
    "    '''\n",
    "    \n",
    "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        \n",
    "def create_train_valid(file):\n",
    "    \n",
    "    '''\n",
    "    Description:\n",
    "    функция принимает тренировочный файл с \n",
    "    обязательными колонками text, extracted_part, label\n",
    "    и разделяет их на train и valid выборки\n",
    "    \n",
    "    input:\n",
    "    file -> directory for save json file\n",
    "    \n",
    "    output:\n",
    "    train_data, valid_data -> list\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    train = pd.read_json(file)\n",
    "        \n",
    "    N = len(train)\n",
    "    valid_idx = np.random.randint(N, size=N//5)\n",
    "    train_idx = list(set(np.arange(N))-set(valid_idx))\n",
    "        \n",
    "    train_data = []\n",
    "    valid_data = []\n",
    "    for i in train_idx:\n",
    "        train_data.append(\n",
    "        [train.loc[i, 'text'], {'entities': [(train.loc[i, 'extracted_part']['answer_start'][0], \n",
    "                                                    train.loc[i, 'extracted_part']['answer_end'][0], \n",
    "                                                    train.loc[i, 'label'])]}]\n",
    "    )\n",
    "    \n",
    "    for i in valid_idx:\n",
    "        valid_data.append(\n",
    "        [train.loc[i, 'text'], {'entities': [(train.loc[i, 'extracted_part']['answer_start'][0], \n",
    "                                                    train.loc[i, 'extracted_part']['answer_end'][0], \n",
    "                                                    train.loc[i, 'label'])]}]\n",
    "    )\n",
    "    \n",
    "    #valid_data = train.loc[valid_idx, ['text', 'label', 'extracted_part']] # data frame\n",
    "    print(f'Размер тренировачной выборки: {len(train_data)}')\n",
    "    print(f'Размер валидационной выборки: {len(valid_data)}')\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dd078fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_train = load_data('data/TRAINING_DATA.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "091af74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировачной выборки: 1479\n",
      "Размер валидационной выборки: 359\n"
     ]
    }
   ],
   "source": [
    "# формируем train/valid выборки\n",
    "train_data, valid_data = create_train_valid('data/main_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85ab2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"ru\")\n",
    "def create_training(data):\n",
    "    \n",
    "    TRAIN_DATA = data\n",
    "    \n",
    "    # идентификация базовой модели для русского языка\n",
    "    \n",
    "    \n",
    "    db = DocBin()\n",
    "    \n",
    "    for text, annot in tqdm(TRAIN_DATA):\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot['entities']:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode='contract')\n",
    "            if span is None:\n",
    "                print('Skipping entity')\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcf4b0c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0434bb4a8b02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# конфертируем тренировочные и обучающие файлы\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#train_model.to_disk('./data/train_data.spacy')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvalid_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# конфертируем тренировочные и обучающие файлы\n",
    "\n",
    "train_model = create_training(train_data)\n",
    "#train_model.to_disk('./data/train_data.spacy')\n",
    "valid_model = create_training(valid_data)\n",
    "#valid_model.to_disk('./data/valid_data.spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be5cbaf",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4024210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, STOPWORDS=stopwords.words('russian')):\n",
    "    \n",
    "    '''\n",
    "    Describe:\n",
    "    Функция приводит данные в надлежащий вид и форму\n",
    "    \n",
    "    input:\n",
    "    text -> str\n",
    "    STOPWORDS -> list of stopwords\n",
    "    \n",
    "    output:\n",
    "    sencente -> list of seqence\n",
    "    '''\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    for sent in sent_tokenize(text):\n",
    "        sentence = preprocess_string(remove_stopwords(sent, stopwords=STOPWORDS))\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def training(model_name):\n",
    "    '''\n",
    "    Description:\n",
    "    Показывает ближайшие слова к заданному слову\n",
    "    \n",
    "    Input:\n",
    "    model_name -> str (path to save model)\n",
    "    \n",
    "    output:\n",
    "    trained word2vec model \n",
    "    '''\n",
    "    \n",
    "    with open('data/work_vec_text.json', \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = json.load(f)\n",
    "    sentences = texts\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    \n",
    "    w2v_model = Word2Vec(min_count=5,\n",
    "                         window=3,\n",
    "                         vector_size=500,\n",
    "                         sample=6e-5,\n",
    "                         alpha=0.03,\n",
    "                         min_alpha=0.0007,\n",
    "                         negative=20,\n",
    "                         workers=cores-1    \n",
    "                        )\n",
    "    w2v_model.build_vocab(texts)\n",
    "    w2v_model.train(texts, \n",
    "                    total_examples=w2v_model.corpus_count,\n",
    "                   epochs=30)\n",
    "    w2v_model.save(f'word_vectors/{model_name}.model')\n",
    "    w2v_model.wv.save_word2vec_format(f'word_vectors/word2vec_{model_name}.txt')\n",
    "    \n",
    "def gensim_similary(word, model_name):\n",
    "    \n",
    "    '''\n",
    "    Description:\n",
    "    Показывает ближайшие слова к заданному слову\n",
    "    \n",
    "    Input:\n",
    "    word - str\n",
    "    model_name -> str (path to txt file)\n",
    "    \n",
    "    output:\n",
    "    list of closest word vectors to a given word\n",
    "    '''\n",
    "    model = KeyedVectors.load_word2vec_format(model_name,\n",
    "                                             binary=False)\n",
    "    results = model.most_similar(positive=[word])\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b55021d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тренируем word_2_vec модель\n",
    "training('contur_train_count_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69c6882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('отдельно', 0.9632689952850342), ('заключаемого', 0.9627524018287659), ('извещении', 0.9575595855712891), ('требование', 0.9559670090675354), ('заявке', 0.9552162289619446), ('прилагается', 0.9540984034538269), ('файлом', 0.9508460760116577), ('составляет', 0.945363461971283), ('прикладывается', 0.9432919025421143), ('предложение', 0.9430250525474548)]\n"
     ]
    }
   ],
   "source": [
    "# отображения ближайших векторов\n",
    "gensim_similary('контракт', 'word_vectors/word2vec_contur_train_count_5.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e78566",
   "metadata": {},
   "source": [
    "# формирование выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16b4cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling(data):\n",
    "    '''\n",
    "    Description:\n",
    "    Функция увеличивает количество записей в 2 раза \n",
    "    переводя весь текст в нижний регистр.\n",
    "    \n",
    "    input:\n",
    "    data -> list\n",
    "    \n",
    "    output:\n",
    "    train_data -> list\n",
    "    '''\n",
    "    \n",
    "    train_data = data\n",
    "    \n",
    "    print(f'Размер тренировочного датасета до upsampling: {len(train_data)}')\n",
    "    \n",
    "    for i in range(len(train_data)):\n",
    "        train_data.append([train_data[0][0].lower(), \n",
    "                         train_data[0][1]])\n",
    "        \n",
    "    print(f'Размер тренировочного датасета после upsampling: {len(train_data)}')\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8c1478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проводим upsampling\n",
    "#train_data = upsampling(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1164e59",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3595e3",
   "metadata": {},
   "source": [
    "def train_spacy(data, iterations):\n",
    "    '''\n",
    "    Description:\n",
    "    Обучение модели nlp для решения задачи NER.\n",
    "        \n",
    "    input:\n",
    "    data -> list\n",
    "    iterations -> int\n",
    "    \n",
    "    output:\n",
    "    nlp -> spacy.lang.ru.Russian\n",
    "    '''\n",
    "    \n",
    "    TRAIN_DATA = data\n",
    "    # идентификация базовой модели для русского языка\n",
    "    nlp = spacy.blank(\"ru\")\n",
    "    # настройка пайплайна обучения\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\", last=True)\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for batch in spacy.util.minibatch(TRAIN_DATA, size=2):\n",
    "                for text, annotations in batch:\n",
    "                    # создание образца\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    nlp.update([example], sgd=optimizer, losses=losses, drop=0.2)\n",
    "                \n",
    "            print(losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "083febe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy_test(data, iterations):\n",
    "    '''\n",
    "    Description:\n",
    "    Обучение модели nlp для решения задачи NER.\n",
    "        \n",
    "    input:\n",
    "    data -> list\n",
    "    iterations -> int\n",
    "    \n",
    "    output:\n",
    "    nlp -> spacy.lang.ru.Russian\n",
    "    '''\n",
    "    \n",
    "    TRAIN_DATA = data\n",
    "    # идентификация базовой модели для русского языка\n",
    "    nlp = spacy.blank(\"ru\")\n",
    "    # настройка пайплайна обучения\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\", last=True)\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                # создание образца\n",
    "                doc = nlp.make_doc(text)\n",
    "                nlp.update((text, annotations),\n",
    "                           sgd=optimizer, \n",
    "                           losses=losses, \n",
    "                           drop=0.2\n",
    "                          )\n",
    "                \n",
    "            print(losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403558eb",
   "metadata": {},
   "source": [
    "## Тренировка через консоль"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9992310",
   "metadata": {},
   "source": [
    "## Инициализация векторов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f1a01ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m[i] Creating blank nlp object for language 'ru'\u001b[0m\n",
      "\u001b[38;5;2m[+] Successfully converted 1423 vectors\u001b[0m\n",
      "\u001b[38;5;2m[+] Saved nlp object with vectors to output directory. You can now use\n",
      "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
      "E:\\DS\\Jupyter_notebook_directory\\Tests progects\\Contur\\models\\word_vectors_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 15:14:40.821432: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-04-16 15:14:40.821500: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-16 15:14:46.168920: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-04-16 15:14:46.169548: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2023-04-16 15:14:46.170104: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2023-04-16 15:14:46.170700: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2023-04-16 15:14:46.226741: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2023-04-16 15:14:46.227313: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2023-04-16 15:14:46.227352: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.fb5ae2tyxyh2ijrdkgdgq3xbklktf43h.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[2023-04-16 15:14:47,740] [INFO] Reading vectors from word_vectors\\word2vec_contur_train_count_5.txt\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "545it [00:00, 5396.10it/s]\n",
      "1111it [00:00, 5518.35it/s]\n",
      "1423it [00:00, 5473.07it/s]\n",
      "[2023-04-16 15:14:48,004] [INFO] Loaded vectors from word_vectors\\word2vec_contur_train_count_5.txt\n"
     ]
    }
   ],
   "source": [
    "# инициализируем вектора\n",
    "!python -m spacy init vectors ru word_vectors/word2vec_contur_train_count_5.txt --name word_vectors_model models/word_vectors_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880259b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m[i] Saving to output directory: models\\04\u001b[0m\n",
      "\u001b[38;5;4m[i] Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m[+] Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m[i] Pipeline: ['ner']\u001b[0m\n",
      "\u001b[38;5;4m[i] Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  --------  ------  ------  ------  ------\n",
      "  0       0    188.40    0.00    0.00    0.00    0.00\n",
      "  0     200   7347.68    1.59    1.51    1.68    0.02\n",
      "  0     400   6984.73    6.70    7.06    6.38    0.07\n",
      "  0     600   1240.24   46.21   50.00   42.95    0.46\n",
      "  0     800    602.40   51.38   62.50   43.62    0.51\n",
      "  0    1000    459.90   54.47   66.83   45.97    0.54\n",
      "  0    1200    524.13   56.00   75.14   44.63    0.56\n",
      "  0    1400    482.12   58.22   54.55   62.42    0.58\n",
      "  1    1600    288.51   53.22   52.44   54.03    0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 06:23:22.718991: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-04-16 06:23:22.719032: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-16 06:23:27.356587: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-04-16 06:23:27.357318: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2023-04-16 06:23:27.358113: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2023-04-16 06:23:27.359416: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2023-04-16 06:23:27.439101: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2023-04-16 06:23:27.439708: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2023-04-16 06:23:27.439748: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.fb5ae2tyxyh2ijrdkgdgq3xbklktf43h.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[2023-04-16 06:23:28,800] [INFO] Set up nlp object from config\n",
      "[2023-04-16 06:23:28,812] [INFO] Pipeline: ['ner']\n",
      "[2023-04-16 06:23:28,816] [INFO] Created vocabulary\n",
      "[2023-04-16 06:23:28,817] [INFO] Finished initializing nlp object\n",
      "[2023-04-16 06:23:37,184] [INFO] Initialized pipeline components: ['ner']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1    1800    316.70   59.24   56.36   62.42    0.59\n",
      "  1    2000    352.77   55.20   56.06   54.36    0.55\n",
      "  1    2200    317.35   64.78   68.54   61.41    0.65\n",
      "  1    2400    498.44   56.70   66.07   49.66    0.57\n",
      "  1    2600    442.04   59.84   56.38   63.76    0.60\n",
      "  1    2800    234.77   63.24   59.01   68.12    0.63\n",
      "  2    3000    467.55   59.01   62.31   56.04    0.59\n",
      "  2    3200    357.80   53.65   48.26   60.40    0.54\n",
      "  2    3400    314.79   67.43   66.13   68.79    0.67\n",
      "  2    3600    330.64   67.39   62.46   73.15    0.67\n",
      "  2    3800    221.36   61.84   67.06   57.38    0.62\n",
      "  2    4000    283.56   65.95   70.34   62.08    0.66\n",
      "  2    4200    265.09   67.90   75.93   61.41    0.68\n",
      "  2    4400    514.21   58.29   60.73   56.04    0.58\n",
      "  3    4600    211.12   68.22   67.66   68.79    0.68\n",
      "  3    4800    349.76   67.01   67.93   66.11    0.67\n",
      "  3    5000    279.51   65.56   65.02   66.11    0.66\n",
      "  3    5200    271.58   65.49   68.89   62.42    0.65\n",
      "  3    5400    228.87   64.71   59.83   70.47    0.65\n",
      "  3    5600    256.64   60.86   54.35   69.13    0.61\n",
      "  3    5800    369.85   65.75   67.13   64.43    0.66\n",
      "  4    6000    252.85   69.12   72.43   66.11    0.69\n",
      "  4    6200    229.05   65.31   66.21   64.43    0.65\n",
      "  4    6400    211.97   68.82   69.90   67.79    0.69\n",
      "  4    6600    186.27   60.72   53.15   70.81    0.61\n",
      "  4    6800    226.50   73.02   76.95   69.46    0.73\n",
      "  4    7000    184.81   68.33   72.73   64.43    0.68\n",
      "  4    7200    215.47   67.80   77.83   60.07    0.68\n",
      "  5    7400    260.20   71.22   69.09   73.49    0.71\n",
      "  5    7600    279.77   70.59   68.79   72.48    0.71\n",
      "  5    7800    148.76   59.92   71.30   51.68    0.60\n",
      "  5    8000    205.62   68.72   70.03   67.45    0.69\n",
      "  5    8200    268.28   62.37   63.01   61.74    0.62\n",
      "  5    8400    237.51   67.13   68.90   65.44    0.67\n",
      "\u001b[38;5;2m[+] Saved pipeline to output directory\u001b[0m\n",
      "models\\04\\model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train models/01/config.cfg --output models/04 --paths.train data/train_data.spacy --paths.dev data/valid_data.spacy --paths.vectors models/01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67701af",
   "metadata": {},
   "source": [
    "# Инициализируем вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d067e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировачной выборки: 1467\n",
      "Размер валидационной выборки: 359\n"
     ]
    }
   ],
   "source": [
    "# формируем train/valid выборки\n",
    "train_data, valid_data = create_train_valid('data/main_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5119ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем \"пустую модель\", но с кастомными векторами\n",
    "#nlp = spacy.load('models/04/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61b3e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('models/word_vectors_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3dd27256",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('models/word_vectors_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a07454",
   "metadata": {},
   "source": [
    "# Тренируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7b60ec65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m[i] Saving to output directory: models\\06\u001b[0m\n",
      "\u001b[38;5;4m[i] Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m[+] Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m[i] Pipeline: ['ner']\u001b[0m\n",
      "\u001b[38;5;4m[i] Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  --------  ------  ------  ------  ------\n",
      "  0       0    188.40    0.00    0.00    0.00    0.00\n",
      "  0     200   7347.68    1.59    1.51    1.68    0.02\n",
      "  0     400   6984.73    6.70    7.06    6.38    0.07\n",
      "  0     600   1240.24   46.21   50.00   42.95    0.46\n",
      "  0     800    602.40   51.38   62.50   43.62    0.51\n",
      "  0    1000    459.90   54.47   66.83   45.97    0.54\n",
      "  0    1200    524.13   56.00   75.14   44.63    0.56\n",
      "  0    1400    482.12   58.22   54.55   62.42    0.58\n",
      "  1    1600    288.51   53.22   52.44   54.03    0.53\n",
      "  1    1800    316.70   59.24   56.36   62.42    0.59\n",
      "  1    2000    352.77   55.20   56.06   54.36    0.55\n",
      "  1    2200    317.35   64.78   68.54   61.41    0.65\n",
      "  1    2400    498.44   56.70   66.07   49.66    0.57\n",
      "  1    2600    442.04   59.84   56.38   63.76    0.60\n",
      "  1    2800    234.77   63.24   59.01   68.12    0.63\n",
      "  2    3000    467.55   59.01   62.31   56.04    0.59\n",
      "  2    3200    357.80   53.65   48.26   60.40    0.54\n",
      "  2    3400    314.79   67.43   66.13   68.79    0.67\n",
      "  2    3600    330.64   67.39   62.46   73.15    0.67\n",
      "  2    3800    221.36   61.84   67.06   57.38    0.62\n",
      "  2    4000    283.56   65.95   70.34   62.08    0.66\n",
      "  2    4200    265.09   67.90   75.93   61.41    0.68\n",
      "  2    4400    514.21   58.29   60.73   56.04    0.58\n",
      "  3    4600    211.12   68.22   67.66   68.79    0.68\n",
      "  3    4800    349.76   67.01   67.93   66.11    0.67\n",
      "  3    5000    279.51   65.56   65.02   66.11    0.66\n",
      "  3    5200    271.58   65.49   68.89   62.42    0.65\n",
      "  3    5400    228.87   64.71   59.83   70.47    0.65\n",
      "  3    5600    256.64   60.86   54.35   69.13    0.61\n",
      "  3    5800    369.85   65.75   67.13   64.43    0.66\n",
      "  4    6000    252.85   69.12   72.43   66.11    0.69\n",
      "  4    6200    229.05   65.31   66.21   64.43    0.65\n",
      "  4    6400    211.97   68.82   69.90   67.79    0.69\n",
      "  4    6600    186.27   60.72   53.15   70.81    0.61\n",
      "  4    6800    226.50   73.02   76.95   69.46    0.73\n",
      "  4    7000    184.81   68.33   72.73   64.43    0.68\n",
      "  4    7200    215.47   67.80   77.83   60.07    0.68\n",
      "  5    7400    260.20   71.22   69.09   73.49    0.71\n",
      "  5    7600    279.77   70.59   68.79   72.48    0.71\n",
      "  5    7800    148.76   59.92   71.30   51.68    0.60\n",
      "  5    8000    205.62   68.72   70.03   67.45    0.69\n",
      "  5    8200    268.28   62.37   63.01   61.74    0.62\n",
      "  5    8400    237.51   67.13   68.90   65.44    0.67\n",
      "\u001b[38;5;2m[+] Saved pipeline to output directory\u001b[0m\n",
      "models\\06\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 15:51:54.250498: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-04-16 15:51:54.250545: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-16 15:51:59.409676: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-04-16 15:51:59.410329: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2023-04-16 15:51:59.410968: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2023-04-16 15:51:59.411594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2023-04-16 15:51:59.477713: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2023-04-16 15:51:59.478452: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2023-04-16 15:51:59.478497: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.fb5ae2tyxyh2ijrdkgdgq3xbklktf43h.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[2023-04-16 15:52:00,973] [INFO] Set up nlp object from config\n",
      "[2023-04-16 15:52:00,986] [INFO] Pipeline: ['ner']\n",
      "[2023-04-16 15:52:00,991] [INFO] Created vocabulary\n",
      "[2023-04-16 15:52:01,224] [INFO] Added vectors: models/word_vectors_model\n",
      "[2023-04-16 15:52:01,231] [INFO] Finished initializing nlp object\n",
      "[2023-04-16 15:52:10,386] [INFO] Initialized pipeline components: ['ner']\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train models/word_vectors_model/config.cfg --output models/06 --paths.train data/train_data.spacy --paths.dev data/valid_data.spacy --paths.vectors models/word_vectors_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c60679",
   "metadata": {},
   "source": [
    "# Test model (validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f8e24",
   "metadata": {},
   "source": [
    "## Оценка точности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88c83360",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('models/06/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27e92105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(row):\n",
    "    text = row['text'][row['extracted_part']['entities'][0][0]:row['extracted_part']['entities'][0][1]]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84cd6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(row):\n",
    "    '''\n",
    "    Description:\n",
    "    Функция возвращает именованную сущность \n",
    "    если таковая существует в иначе возвращает пустую строку\n",
    "    \n",
    "    input:\n",
    "    row -> str\n",
    "    \n",
    "    output:\n",
    "    ent.text -> str\n",
    "    '''\n",
    "    \n",
    "    if len(nlp(row).ents) > 0:\n",
    "        return nlp(row).ents[0]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eaaf2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем датасет для оценки точности\n",
    "valid = pd.DataFrame(valid_data, columns=['text', 'extracted_part'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6799aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем колонку с текстом\n",
    "valid['extracted_text'] = valid.apply(lambda row: extract_text(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "45381217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем колонку ответами\n",
    "valid['predict'] = valid['text'].apply(lambda row: str(predict_text(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3f00870b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.754874651810585"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# точность предсказаний\n",
    "accuracy_score(valid['extracted_text'], valid['predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec9215",
   "metadata": {},
   "source": [
    "# Результаты исследований\n",
    "\n",
    "Модель и её результаты:\n",
    "\n",
    "## work_model_30_iter (надо переобучить)\n",
    "\n",
    "Допущена ошибка и обучающая выборка не была разбита на train/valid\n",
    "\n",
    "**Результаты на valid:**\n",
    "\n",
    "Accuracy: 0.84 (анулированы из-за ошибки)\n",
    "\n",
    "\n",
    "## work_model_upsampling_30_iter\n",
    "\n",
    "**Пайплайн:**\n",
    "* Разбить выборку на train/valid\n",
    "\n",
    "* Провести **upsampling** на train выборке\n",
    "\n",
    "**Результаты на valid:**\n",
    "\n",
    "Accuracy: 0.6573816155988857\n",
    "\n",
    "## work_model_30_iter_train_valid\n",
    "\n",
    "**Пайплайн:**\n",
    "* Разбить выборку на train/valid\n",
    "\n",
    "**Результаты на valid:**\n",
    "\n",
    "Accuracy: 0.7103064066852368\n",
    "\n",
    "## models/02/model-best\n",
    "\n",
    "**Пайплайн**\n",
    "* Предварительно обучили w2v модель и интегрировали кастомные вектора в нашу модель.\n",
    "\n",
    "* Разбили выборку на  train/valid \n",
    "\n",
    "* запустили скрипт:\n",
    "!python -m spacy train models/01/config.cfg --output models/02 --paths.train models/data/train_data.spacy --paths.dev models/data/valid_data.spacy --paths.vectors models/01\n",
    "\n",
    "* batch_size = 1000\n",
    "\n",
    "**Результаты на valid:**\n",
    "\n",
    "Accuracy: 0.7214484679665738\n",
    "\n",
    "## models/02/model-best\n",
    "\n",
    "**Пайплайн**\n",
    "* Предварительно обучили w2v модель и интегрировали кастомные вектора в нашу модель.\n",
    "\n",
    "* Разбили выборку на  train/valid \n",
    "\n",
    "* запустили скрипт:\n",
    "!python -m spacy train models/01/config.cfg --output models/02 --paths.train models/data/train_data.spacy --paths.dev models/data/valid_data.spacy --paths.vectors models/01\n",
    "\n",
    "* batch_size = 2\n",
    "\n",
    "**Результаты на valid:**\n",
    "\n",
    "Accuracy: 0.7214484679665738\n",
    "\n",
    "\n",
    "\n",
    "## models/03/model-best\n",
    "\n",
    "**Пайплайн**\n",
    "* Предварительно обучили w2v модель и интегрировали кастомные вектора в нашу модель.\n",
    "\n",
    "* Разбили выборку на  train/valid \n",
    "\n",
    "* запустили скрипт:\n",
    "!python -m spacy train models/01/config.cfg --output models/02 --paths.train models/data/train_data.spacy --paths.dev models/data/valid_data.spacy --paths.vectors models/01\n",
    "\n",
    "* window_size = 1 >>> window_size = 2\n",
    "\n",
    "**Результаты на valid:**\n",
    "\n",
    "Accuracy: 0.72\n",
    "\n",
    "## models/04/model-best\n",
    "\n",
    "**Пайплайн**\n",
    "* Предварительно обучили w2v модель и интегрировали кастомные вектора в нашу модель.\n",
    "\n",
    "* Разбили выборку на  train/valid \n",
    "\n",
    "* запустили скрипт:\n",
    "!python -m spacy train models/01/config.cfg --output models/02 --paths.train models/data/train_data.spacy --paths.dev models/data/valid_data.spacy\n",
    "\n",
    "* удалили кастомные вектора из обучающего пайплайна \n",
    "\n",
    "remove: --paths.vectors models/01 \n",
    "\n",
    "**Результаты на valid:**\n",
    "\n",
    "Accuracy: 0.754874651810585\n",
    "\n",
    "## models/05/model-best\n",
    "\n",
    "**Пайплайн**\n",
    "* Предварительно обучили w2v модель и интегрировали кастомные вектора в нашу модель.\n",
    "\n",
    "* Разбили выборку на  train/valid \n",
    "\n",
    "* запустили скрипт:\n",
    "\n",
    "* * инициилизировали вектора\n",
    "\n",
    "!python -m spacy init vectors ru word_vectors/word2vec_contur_train_count_5.txt --name word_vectors_model models/word_vectors_model \n",
    "\n",
    "* * обучили модель \n",
    "\n",
    "!python -m spacy train models/word_vectors_model/config.cfg --output models/05 --paths.train data/train_data.spacy --paths.dev data/valid_data.spacy --paths.vectors models/word_vectors_model\n",
    "\n",
    "**Результаты на valid:**\n",
    "\n",
    "Accuracy: 0.7576601671309192\n",
    "\n",
    "## models/06/model-best\n",
    "\n",
    "**Пайплайн**\n",
    "* Предварительно обучили w2v модель и интегрировали кастомные вектора в нашу модель.\n",
    "\n",
    "* Разбили выборку на  train/valid \n",
    "\n",
    "* запустили скрипт:\n",
    "\n",
    "* * инициилизировали вектора\n",
    "\n",
    "!python -m spacy init vectors ru word_vectors/word2vec_contur_train_count_5.txt --name word_vectors_model models/word_vectors_model \n",
    "\n",
    "* * обучили модель \n",
    "\n",
    "!python -m spacy train models/word_vectors_model/config.cfg --output models/05 --paths.train data/train_data.spacy --paths.dev data/valid_data.spacy --paths.vectors models/word_vectors_model\n",
    "\n",
    "поменяли количество эпох на 30 \n",
    "\n",
    "**Результаты на valid:**\n",
    "\n",
    "Accuracy: 0.754874651810585"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
